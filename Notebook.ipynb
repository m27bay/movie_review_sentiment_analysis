{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Mining Project."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://ai.stanford.edu/~amaas/data/sentiment/"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # array, ...\n",
    "import pandas as pd # DataFrame, ...\n",
    "import matplotlib as plt # countplot, barplot, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re # regex.\n",
    "import os # files and directories manipulation.\n",
    "import codecs # open files.\n",
    "import string # translate, maketrans\n",
    "import collections\n",
    "import copy # deepcopy, ...\n",
    "from enum import Enum\n",
    "import sys\n",
    "from statistics import mean\n",
    "from random import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove HTML tags.\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\micka\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\micka\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\micka\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "import nltk\n",
    "\n",
    "# Frequency distribution.\n",
    "from nltk import FreqDist\n",
    "\n",
    "# Wordnet.\n",
    "from nltk.corpus import wordnet as wn\n",
    "nltk.download('omw-1.4')\n",
    "# from PyDictionary import PyDictionary\n",
    "# dictionary=PyDictionary()\n",
    "\n",
    "# nltk stopwords.\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# lemmatization.\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Tokenization.\n",
    "from nltk.tokenize import WordPunctTokenizer, word_tokenize\n",
    "\n",
    "# Words nature.\n",
    "from nltk.tag import pos_tag\n",
    "\n",
    "# Sentiment.\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "nltk.download('vader_lexicon')\n",
    "sia = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sklearn classifiers.\n",
    "from sklearn.naive_bayes import (\n",
    "    BernoulliNB,\n",
    "    ComplementNB,\n",
    "    MultinomialNB,\n",
    ")\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classes."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean_arg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Clean_arg(Enum):\n",
    "\t\"\"\"\n",
    "\t#! Documentation\n",
    "\t\"\"\"\n",
    "\n",
    "\tHTML = 1\n",
    "\tPUNCTUATION = 2\n",
    "\tNUMBER = 3\n",
    "\tSTOPWORDS = 4\n",
    "\tUSLESS = 5\n",
    "\tLEMMA = 6\n",
    "\tLOWERCASE = 7"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Document():\n",
    "\t\"\"\"\n",
    "\t#! Documentation\n",
    "\t\"\"\"\n",
    "\t\n",
    "\tdef __init__(self, id: int, compound, rating: int, text: str):\n",
    "\t\tself.id = id\n",
    "\t\tself.compound = compound\n",
    "\t\tself.rating = rating\n",
    "\t\tself.text = text\n",
    "\n",
    "\t\tself.words = []\n",
    "\t\tself.tags = []\n",
    "\n",
    "\tdef str(self, arg: str):\n",
    "\t\tif arg == \"token\":\n",
    "\t\t\twords = \", \".join([str(word) for word in self.words])\n",
    "\t\t\treturn \"id : {}, rating : {}, words : {}\".format(self.id, self.rating, words)\n",
    "\t\telif arg == \"tag\":\n",
    "\t\t\ttags = \", \".join([\"({}, {})\".format(word, tag) for word, tag in zip(self.words, self.tags)])\n",
    "\t\t\treturn \"id : {}, rating : {}, tags : {}\".format(self.id, self.rating, tags)\n",
    "\t\telif arg == \"text\":\n",
    "\t\t\treturn \"id : {}, rating : {}, text : {}\".format(self.id, self.rating, self.text)\n",
    "\t\telse:\n",
    "\t\t\treturn \"Unknown argument\"\n",
    "\n",
    "\tdef tokenize(self):\n",
    "\t\tself.words = word_tokenize(self.text)\n",
    "\n",
    "\tdef get_tag(self):\n",
    "\t\tself.tags = [tag for _, tag in pos_tag(self.words)]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## clean_text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def clean_text(_set : list, clean_arg : Clean_arg):\n",
    "\t\"\"\"\n",
    "\t#! Documentation.\n",
    "\t\"\"\"\n",
    "\n",
    "\t#\t\t\n",
    "\ttexts_cleaned = 0\n",
    "\n",
    "\t# Loop for texts.\n",
    "\tfor i in range(len(_set)):\n",
    "\t\t\n",
    "\t\t# Saving informations.\n",
    "\t\tdoc = _set[i]\n",
    "\t\ttext = old_text = doc.text\n",
    "\t\twords = doc.words\n",
    "\t\ttags = doc.tags\n",
    "\n",
    "\t\t# Remove HTML tags.\n",
    "\t\tif clean_arg == Clean_arg.HTML:\n",
    "\t\t\ttext = BeautifulSoup(text, \"html.parser\").text\n",
    "\t\t\n",
    "\t\t# Remove punctuations.\n",
    "\t\telif clean_arg == Clean_arg.PUNCTUATION:\n",
    "\t\t\ttext = text.translate(str.maketrans('', '', string.punctuation))\n",
    "\t\t\n",
    "\t\t# Remove numbers.\n",
    "\t\telif clean_arg == Clean_arg.NUMBER:\n",
    "\t\t\twords = text.split()\n",
    "\t\t\ttext = ' '.join([word for word in words if not word.isnumeric()])\n",
    "\n",
    "\t\t# Remove stop words.\n",
    "\t\telif clean_arg == Clean_arg.STOPWORDS:\n",
    "\t\t\twords_tmp = []\n",
    "\t\t\ttags_tmp = []\n",
    "\n",
    "\t\t\t#\n",
    "\t\t\tfor index, word in enumerate(words):\n",
    "\t\t\t\tif word.lower() not in stop_words:\n",
    "\t\t\t\t\twords_tmp.append(word)\n",
    "\t\t\t\t\ttry:\n",
    "\t\t\t\t\t\ttag = tags[index]\n",
    "\t\t\t\t\t\ttags_tmp.append(tag)\n",
    "\t\t\t\t\texcept IndexError:\n",
    "\t\t\t\t\t\tprint(\"ERROR INDEX : id : {}, word : {}, index : {}, len(tags) : {}\".format(id, word, index, len(tags)))\n",
    "\t\t\t\n",
    "\t\t\twords = words_tmp\n",
    "\t\t\ttags = tags_tmp\n",
    "\n",
    "\t\t#\n",
    "\t\telif clean_arg == Clean_arg.USLESS:\n",
    "\t\t\twords_tmp = []\n",
    "\t\t\ttags_tmp = []\n",
    "\t\t\tusless_tags = [\"NNS\", \"NNP\", \"NNPS\"]\n",
    "\t\t\tusless_words = [\"story\", \"movie\", \"film\"]\n",
    "\n",
    "\t\t\t#\n",
    "\t\t\tfor index, (word, tag) in enumerate(zip(words, tags)):\n",
    "\t\t\t\tif tag not in usless_tags and word.lower() not in usless_words:\n",
    "\t\t\t\t\twords_tmp.append(word)\n",
    "\t\t\t\t\ttags_tmp.append(tag)\n",
    "\n",
    "\t\t\twords = words_tmp\n",
    "\t\t\ttags = tags_tmp\n",
    "\t\t\n",
    "\t\t#\n",
    "\t\telif clean_arg == Clean_arg.LOWERCASE:\n",
    "\t\t\twords = [word.lower() for word in words]\n",
    "\n",
    "\t\t# Remove LEMMA.\n",
    "\t\telif clean_arg == Clean_arg.LEMMA:\n",
    "\t\t\twords_tmp = []\n",
    "\t\t\ttags_tmp = []\n",
    "\t\t\tverbes_forms = [\"VBP\", \"VBN\", \"VBG\", \"VBD\", \"VB\"]\n",
    "\n",
    "\t\t\tfor index, (word, tag) in enumerate(zip(words, tags)):\n",
    "\t\t\t\t# lemmetize verbes.\n",
    "\t\t\t\tif tag in verbes_forms:\n",
    "\t\t\t\t\tverb = lemmatizer.lemmatize(word, pos = \"v\")\n",
    "\t\t\t\t\twords_tmp.append(verb)\n",
    "\t\t\t\t\ttags_tmp.append(\"VBZ\")\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\twords_tmp.append(lemmatizer.lemmatize(word))\n",
    "\t\t\t\t\ttags_tmp.append(tag)\n",
    "\n",
    "\t\t\twords = words_tmp\n",
    "\t\t\ttags = tags_tmp\t\t\t\n",
    "\n",
    "\t\t# Update value.\n",
    "\t\t_set[i] = Document(doc.id, doc.rating, doc.compound, text)\n",
    "\t\t_set[i].words = copy.deepcopy(words)\n",
    "\t\t_set[i].tags = copy.deepcopy(tags)\n",
    "\n",
    "\n",
    "\t\t# Counting modifications.\n",
    "\t\tif old_text != text:\n",
    "\t\t\ttexts_cleaned += 1\n",
    "\n",
    "\t# Printing.\n",
    "\tif clean_arg not in [Clean_arg.STOPWORDS, Clean_arg.LEMMA, Clean_arg.LOWERCASE, Clean_arg.USLESS]:\n",
    "\t\tprint(\"{} texts cleaned\".format(texts_cleaned))\n",
    "\treturn _set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_positive(rating: int):\n",
    "\treturn rating >= 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_negative(rating: int):\n",
    "\treturn rating <= 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(doc, top_100_positive):\n",
    "    features = {}\n",
    "    wordcount = 0\n",
    "    compound_scores = []\n",
    "    positive_scores = []\n",
    "\n",
    "    for word in doc.words:\n",
    "        if word in top_100_positive:\n",
    "            wordcount += 1\n",
    "    \n",
    "    compound_scores.append(doc.compound)\n",
    "    positive_scores.append(doc.rating)\n",
    "\n",
    "    # Adding 1 to the final compound score to always have positive numbers\n",
    "    # since some classifiers you'll use later don't work with negative numbers.\n",
    "    features[\"mean_compound\"] = mean(compound_scores) + 1\n",
    "    features[\"mean_positive\"] = mean(positive_scores)\n",
    "    features[\"wordcount\"] = wordcount\n",
    "\n",
    "    return features"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Files reading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set: train\n",
      "str_rating: neg\n",
      "path: ./data/train/neg\n",
      "file_count : 12500\n",
      "str_rating: pos\n",
      "path: ./data/train/pos\n",
      "file_count : 12500\n",
      "set: test\n",
      "str_rating: neg\n",
      "path: ./data/test/neg\n",
      "file_count : 12500\n",
      "str_rating: pos\n",
      "path: ./data/test/pos\n",
      "file_count : 12500\n"
     ]
    }
   ],
   "source": [
    "    # Stock trainset and testset texts.\n",
    "    docs_train = []\n",
    "    docs_test = []\n",
    "\n",
    "    # Directories.\n",
    "    sets = [\"train\", \"test\"]\n",
    "    str_ratings = [\"neg\", \"pos\"]\n",
    "\n",
    "    # Loop for training and testing directories.\n",
    "    for _set in sets:\n",
    "        print(\"set: {}\".format(_set))\n",
    "\n",
    "        # Loop for \"neg\" and \"pos\" directories.\n",
    "        for str_rating in str_ratings:\n",
    "\n",
    "            # Creaeting path.\n",
    "            path = f\"./data/{_set}/{str_rating}\"\n",
    "\n",
    "            # \n",
    "            print(\"str_rating: {}\".format(str_rating))\n",
    "            print(\"path: {}\".format(path))\n",
    "\n",
    "            # Files counter.\n",
    "            file_count = 0\n",
    "\n",
    "            # Loop for files.\n",
    "            for filename in os.listdir(path):\n",
    "                \n",
    "                file = os.path.join(path, filename)\n",
    "                if os.path.isfile(file):\n",
    "                    with codecs.open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "                        \n",
    "                        # File name parsing to get id and rating.\n",
    "                        split_extension = filename.split(\".\")\n",
    "                        split_id_rating = split_extension[0].split(\"_\")\n",
    "                        id_str = split_id_rating[0]\n",
    "                        rating_str = split_id_rating[1]\n",
    "                        id = rating = -1\n",
    "\n",
    "                        try:\n",
    "                            id = int(id_str)\n",
    "                        except ValueError:\n",
    "                            sys.exit(\"Error casting id to int\")\n",
    "\n",
    "                        try:\n",
    "                            rating = int(rating_str)\n",
    "                        except ValueError:\n",
    "                            sys.exit(\"Error casting rating to int\")\n",
    "\n",
    "                        text = f.read()\n",
    "                        doc = Document(id, rating, sia.polarity_scores(text)[\"compound\"], text)\n",
    "                        if _set == \"train\":\n",
    "                            docs_train.append(doc)\n",
    "                        else:\n",
    "                            docs_test.append(doc)\n",
    "                        file_count += 1\n",
    "                        \n",
    "            print(\"file_count : {}\".format(file_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('train',\n",
       " \"id : 0, rating : 0.7003, text : Story of a man who has unnatural feelings for a pig. Starts out with a opening scene that is a terrific example of absurd comedy. A formal orchestra audience is turned into an insane, violent mob by the crazy chantings of it's singers. Unfortunately it stays absurd the WHOLE time with no general narrative eventually making it just too off putting. Even those from the era should be turned off. The cryptic dialogue would make Shakespeare seem easy to a third grader. On a technical level it's better than you might think with some good cinematography by future great Vilmos Zsigmond. Future stars Sally Kirkland and Frederic Forrest can be seen briefly.\",\n",
       " 'test',\n",
       " \"id : 0, rating : -0.5349, text : Once again Mr. Costner has dragged out a movie for far longer than necessary. Aside from the terrific sea rescue sequences, of which there are very few I just did not care about any of the characters. Most of us have ghosts in the closet, and Costner's character are realized early on, and then forgotten until much later, by which time I did not care. The character we should really care about is a very cocky, overconfident Ashton Kutcher. The problem is he comes off as kid who thinks he's better than anyone else around him and shows no signs of a cluttered closet. His only obstacle appears to be winning over Costner. Finally when we are well past the half way point of this stinker, Costner tells us all about Kutcher's ghosts. We are told why Kutcher is driven to be the best with no prior inkling or foreshadowing. No magic here, it was all I could do to keep from turning it off an hour in.\")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"train\", [word.str(\"text\") for word in docs_train][0], \"test\", [word.str(\"text\") for word in docs_test][0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text mining process."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text cleanup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_train_cleaned, docs_test_cleaned = copy.deepcopy(docs_train), copy.deepcopy(docs_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove HTML tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Programmation\\anaconda\\lib\\site-packages\\bs4\\__init__.py:435: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14669 texts cleaned\n"
     ]
    }
   ],
   "source": [
    "docs_train_cleaned = clean_text(docs_train_cleaned, Clean_arg.HTML)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14537 texts cleaned\n"
     ]
    }
   ],
   "source": [
    "docs_test_cleaned = clean_text(docs_test_cleaned, Clean_arg.HTML)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('train',\n",
       " \"id : 0, rating : 3, text : Story of a man who has unnatural feelings for a pig. Starts out with a opening scene that is a terrific example of absurd comedy. A formal orchestra audience is turned into an insane, violent mob by the crazy chantings of it's singers. Unfortunately it stays absurd the WHOLE time with no general narrative eventually making it just too off putting. Even those from the era should be turned off. The cryptic dialogue would make Shakespeare seem easy to a third grader. On a technical level it's better than you might think with some good cinematography by future great Vilmos Zsigmond. Future stars Sally Kirkland and Frederic Forrest can be seen briefly.\",\n",
       " 'test',\n",
       " \"id : 0, rating : 2, text : Once again Mr. Costner has dragged out a movie for far longer than necessary. Aside from the terrific sea rescue sequences, of which there are very few I just did not care about any of the characters. Most of us have ghosts in the closet, and Costner's character are realized early on, and then forgotten until much later, by which time I did not care. The character we should really care about is a very cocky, overconfident Ashton Kutcher. The problem is he comes off as kid who thinks he's better than anyone else around him and shows no signs of a cluttered closet. His only obstacle appears to be winning over Costner. Finally when we are well past the half way point of this stinker, Costner tells us all about Kutcher's ghosts. We are told why Kutcher is driven to be the best with no prior inkling or foreshadowing. No magic here, it was all I could do to keep from turning it off an hour in.\")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"train\", [word.str(\"text\") for word in docs_train_cleaned][0], \"test\", [word.str(\"text\") for word in docs_test_cleaned][0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24999 texts cleaned\n"
     ]
    }
   ],
   "source": [
    "docs_train_cleaned = clean_text(docs_train_cleaned, Clean_arg.PUNCTUATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24996 texts cleaned\n"
     ]
    }
   ],
   "source": [
    "docs_test_cleaned = clean_text(docs_test_cleaned, Clean_arg.PUNCTUATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('train',\n",
       " 'id : 0, rating : 0.7003, text : Story of a man who has unnatural feelings for a pig Starts out with a opening scene that is a terrific example of absurd comedy A formal orchestra audience is turned into an insane violent mob by the crazy chantings of its singers Unfortunately it stays absurd the WHOLE time with no general narrative eventually making it just too off putting Even those from the era should be turned off The cryptic dialogue would make Shakespeare seem easy to a third grader On a technical level its better than you might think with some good cinematography by future great Vilmos Zsigmond Future stars Sally Kirkland and Frederic Forrest can be seen briefly',\n",
       " 'test',\n",
       " 'id : 0, rating : -0.5349, text : Once again Mr Costner has dragged out a movie for far longer than necessary Aside from the terrific sea rescue sequences of which there are very few I just did not care about any of the characters Most of us have ghosts in the closet and Costners character are realized early on and then forgotten until much later by which time I did not care The character we should really care about is a very cocky overconfident Ashton Kutcher The problem is he comes off as kid who thinks hes better than anyone else around him and shows no signs of a cluttered closet His only obstacle appears to be winning over Costner Finally when we are well past the half way point of this stinker Costner tells us all about Kutchers ghosts We are told why Kutcher is driven to be the best with no prior inkling or foreshadowing No magic here it was all I could do to keep from turning it off an hour in')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"train\", [word.str(\"text\") for word in docs_train_cleaned][0], \"test\", [word.str(\"text\") for word in docs_test_cleaned][0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15331 texts cleaned\n"
     ]
    }
   ],
   "source": [
    "docs_train_cleaned = clean_text(docs_train_cleaned, Clean_arg.NUMBER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14991 texts cleaned\n"
     ]
    }
   ],
   "source": [
    "docs_test_cleaned = clean_text(docs_test_cleaned, Clean_arg.NUMBER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('train',\n",
       " 'id : 0, rating : 3, text : Story of a man who has unnatural feelings for a pig Starts out with a opening scene that is a terrific example of absurd comedy A formal orchestra audience is turned into an insane violent mob by the crazy chantings of its singers Unfortunately it stays absurd the WHOLE time with no general narrative eventually making it just too off putting Even those from the era should be turned off The cryptic dialogue would make Shakespeare seem easy to a third grader On a technical level its better than you might think with some good cinematography by future great Vilmos Zsigmond Future stars Sally Kirkland and Frederic Forrest can be seen briefly',\n",
       " 'test',\n",
       " 'id : 0, rating : 2, text : Once again Mr Costner has dragged out a movie for far longer than necessary Aside from the terrific sea rescue sequences of which there are very few I just did not care about any of the characters Most of us have ghosts in the closet and Costners character are realized early on and then forgotten until much later by which time I did not care The character we should really care about is a very cocky overconfident Ashton Kutcher The problem is he comes off as kid who thinks hes better than anyone else around him and shows no signs of a cluttered closet His only obstacle appears to be winning over Costner Finally when we are well past the half way point of this stinker Costner tells us all about Kutchers ghosts We are told why Kutcher is driven to be the best with no prior inkling or foreshadowing No magic here it was all I could do to keep from turning it off an hour in')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"train\", [word.str(\"text\") for word in docs_train_cleaned][0], \"test\", [word.str(\"text\") for word in docs_test_cleaned][0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_train_prepro_token, docs_test_prepro_token = copy.deepcopy(docs_train_cleaned), copy.deepcopy(docs_test_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in docs_train_prepro_token:\n",
    "\tdoc.tokenize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in docs_test_prepro_token:\n",
    "\tdoc.tokenize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_train, docs_test = docs_train_prepro_token, docs_test_prepro_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('train',\n",
       " 'id : 0, rating : 3, words : Story, of, a, man, who, has, unnatural, feelings, for, a, pig, Starts, out, with, a, opening, scene, that, is, a, terrific, example, of, absurd, comedy, A, formal, orchestra, audience, is, turned, into, an, insane, violent, mob, by, the, crazy, chantings, of, its, singers, Unfortunately, it, stays, absurd, the, WHOLE, time, with, no, general, narrative, eventually, making, it, just, too, off, putting, Even, those, from, the, era, should, be, turned, off, The, cryptic, dialogue, would, make, Shakespeare, seem, easy, to, a, third, grader, On, a, technical, level, its, better, than, you, might, think, with, some, good, cinematography, by, future, great, Vilmos, Zsigmond, Future, stars, Sally, Kirkland, and, Frederic, Forrest, can, be, seen, briefly',\n",
       " 'test',\n",
       " 'id : 0, rating : 2, words : Once, again, Mr, Costner, has, dragged, out, a, movie, for, far, longer, than, necessary, Aside, from, the, terrific, sea, rescue, sequences, of, which, there, are, very, few, I, just, did, not, care, about, any, of, the, characters, Most, of, us, have, ghosts, in, the, closet, and, Costners, character, are, realized, early, on, and, then, forgotten, until, much, later, by, which, time, I, did, not, care, The, character, we, should, really, care, about, is, a, very, cocky, overconfident, Ashton, Kutcher, The, problem, is, he, comes, off, as, kid, who, thinks, hes, better, than, anyone, else, around, him, and, shows, no, signs, of, a, cluttered, closet, His, only, obstacle, appears, to, be, winning, over, Costner, Finally, when, we, are, well, past, the, half, way, point, of, this, stinker, Costner, tells, us, all, about, Kutchers, ghosts, We, are, told, why, Kutcher, is, driven, to, be, the, best, with, no, prior, inkling, or, foreshadowing, No, magic, here, it, was, all, I, could, do, to, keep, from, turning, it, off, an, hour, in')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"train\", [word.str(\"token\") for word in docs_train][0], \"test\", [word.str(\"token\") for word in docs_test][0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get words tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_train_tags, docs_test_tags = copy.deepcopy(docs_train), copy.deepcopy(docs_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in docs_train_tags:\n",
    "\tdoc.get_tag()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32md:\\Programmation\\anaconda\\lib\\genericpath.py:30\u001b[0m, in \u001b[0;36misfile\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 30\u001b[0m     st \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39;49mstat(path)\n\u001b[0;32m     31\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mOSError\u001b[39;00m, \u001b[39mValueError\u001b[39;00m):\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 2] Le fichier spécifié est introuvable: 'C:\\\\Users\\\\micka/nltk_data'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\Mickael\\Cours\\UVSQ\\M2\\S2\\fouille_de_donnees_et_analyse_predictive\\movie_review_sentiment_analysis\\Notebook.ipynb Cellule 48\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Mickael/Cours/UVSQ/M2/S2/fouille_de_donnees_et_analyse_predictive/movie_review_sentiment_analysis/Notebook.ipynb#X64sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfor\u001b[39;00m doc \u001b[39min\u001b[39;00m docs_test_tags:\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Mickael/Cours/UVSQ/M2/S2/fouille_de_donnees_et_analyse_predictive/movie_review_sentiment_analysis/Notebook.ipynb#X64sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \tdoc\u001b[39m.\u001b[39;49mget_tag()\n",
      "\u001b[1;32md:\\Mickael\\Cours\\UVSQ\\M2\\S2\\fouille_de_donnees_et_analyse_predictive\\movie_review_sentiment_analysis\\Notebook.ipynb Cellule 48\u001b[0m in \u001b[0;36mDocument.get_tag\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Mickael/Cours/UVSQ/M2/S2/fouille_de_donnees_et_analyse_predictive/movie_review_sentiment_analysis/Notebook.ipynb#X64sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_tag\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Mickael/Cours/UVSQ/M2/S2/fouille_de_donnees_et_analyse_predictive/movie_review_sentiment_analysis/Notebook.ipynb#X64sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m \t\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtags \u001b[39m=\u001b[39m [tag \u001b[39mfor\u001b[39;00m _, tag \u001b[39min\u001b[39;00m pos_tag(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwords)]\n",
      "File \u001b[1;32md:\\Programmation\\anaconda\\lib\\site-packages\\nltk\\tag\\__init__.py:165\u001b[0m, in \u001b[0;36mpos_tag\u001b[1;34m(tokens, tagset, lang)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpos_tag\u001b[39m(tokens, tagset\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, lang\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39meng\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m    141\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    142\u001b[0m \u001b[39m    Use NLTK's currently recommended part of speech tagger to\u001b[39;00m\n\u001b[0;32m    143\u001b[0m \u001b[39m    tag the given list of tokens.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    163\u001b[0m \u001b[39m    :rtype: list(tuple(str, str))\u001b[39;00m\n\u001b[0;32m    164\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 165\u001b[0m     tagger \u001b[39m=\u001b[39m _get_tagger(lang)\n\u001b[0;32m    166\u001b[0m     \u001b[39mreturn\u001b[39;00m _pos_tag(tokens, tagset, tagger, lang)\n",
      "File \u001b[1;32md:\\Programmation\\anaconda\\lib\\site-packages\\nltk\\tag\\__init__.py:107\u001b[0m, in \u001b[0;36m_get_tagger\u001b[1;34m(lang)\u001b[0m\n\u001b[0;32m    105\u001b[0m     tagger\u001b[39m.\u001b[39mload(ap_russian_model_loc)\n\u001b[0;32m    106\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 107\u001b[0m     tagger \u001b[39m=\u001b[39m PerceptronTagger()\n\u001b[0;32m    108\u001b[0m \u001b[39mreturn\u001b[39;00m tagger\n",
      "File \u001b[1;32md:\\Programmation\\anaconda\\lib\\site-packages\\nltk\\tag\\perceptron.py:167\u001b[0m, in \u001b[0;36mPerceptronTagger.__init__\u001b[1;34m(self, load)\u001b[0m\n\u001b[0;32m    164\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclasses \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m()\n\u001b[0;32m    165\u001b[0m \u001b[39mif\u001b[39;00m load:\n\u001b[0;32m    166\u001b[0m     AP_MODEL_LOC \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mfile:\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(\n\u001b[1;32m--> 167\u001b[0m         find(\u001b[39m\"\u001b[39;49m\u001b[39mtaggers/averaged_perceptron_tagger/\u001b[39;49m\u001b[39m\"\u001b[39;49m \u001b[39m+\u001b[39;49m PICKLE)\n\u001b[0;32m    168\u001b[0m     )\n\u001b[0;32m    169\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mload(AP_MODEL_LOC)\n",
      "File \u001b[1;32md:\\Programmation\\anaconda\\lib\\site-packages\\nltk\\data.py:522\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    519\u001b[0m \u001b[39m# Check each item in our path\u001b[39;00m\n\u001b[0;32m    520\u001b[0m \u001b[39mfor\u001b[39;00m path_ \u001b[39min\u001b[39;00m paths:\n\u001b[0;32m    521\u001b[0m     \u001b[39m# Is the path item a zipfile?\u001b[39;00m\n\u001b[1;32m--> 522\u001b[0m     \u001b[39mif\u001b[39;00m path_ \u001b[39mand\u001b[39;00m (os\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49misfile(path_) \u001b[39mand\u001b[39;00m path_\u001b[39m.\u001b[39mendswith(\u001b[39m\"\u001b[39m\u001b[39m.zip\u001b[39m\u001b[39m\"\u001b[39m)):\n\u001b[0;32m    523\u001b[0m         \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    524\u001b[0m             \u001b[39mreturn\u001b[39;00m ZipFilePathPointer(path_, resource_name)\n",
      "File \u001b[1;32md:\\Programmation\\anaconda\\lib\\genericpath.py:30\u001b[0m, in \u001b[0;36misfile\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[39m\"\"\"Test whether a path is a regular file\"\"\"\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 30\u001b[0m     st \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39;49mstat(path)\n\u001b[0;32m     31\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mOSError\u001b[39;00m, \u001b[39mValueError\u001b[39;00m):\n\u001b[0;32m     32\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for doc in docs_test_tags:\n",
    "\tdoc.get_tag()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"train\", [word.str(\"tag\") for word in docs_train_tags][0], \"test\", [word.str(\"tag\") for word in docs_test_tags][0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop words removal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_train_stop_words = copy.deepcopy(docs_train_tags)\n",
    "docs_test_stop_words = copy.deepcopy(docs_test_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "debug"
    ]
   },
   "outputs": [],
   "source": [
    "docs_train_stop_words = clean_text(docs_train_stop_words, Clean_arg.STOPWORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "debug"
    ]
   },
   "outputs": [],
   "source": [
    "docs_test_stop_words = clean_text(docs_test_stop_words, Clean_arg.STOPWORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"train\", [word.str(\"token\") for word in docs_train_stop_words][0], \"test\", [word.str(\"token\") for word in docs_test_stop_words][0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove usless tags / word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_train_rem_usless = copy.deepcopy(docs_train_stop_words)\n",
    "docs_test_rem_usless = copy.deepcopy(docs_test_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "debug"
    ]
   },
   "outputs": [],
   "source": [
    "docs_train_rem_usless = clean_text(docs_train_rem_usless, Clean_arg.USLESS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "debug"
    ]
   },
   "outputs": [],
   "source": [
    "docs_test_rem_usless = clean_text(docs_test_rem_usless, Clean_arg.USLESS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"train\", [word.str(\"token\") for word in docs_train_rem_usless][0], \"test\", [word.str(\"token\") for word in docs_test_rem_usless][0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_train_lemma = copy.deepcopy(docs_train_rem_usless)\n",
    "docs_test_lemma = copy.deepcopy(docs_test_rem_usless)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "debug"
    ]
   },
   "outputs": [],
   "source": [
    "docs_train_lemma = clean_text(docs_train_lemma, Clean_arg.LEMMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "debug"
    ]
   },
   "outputs": [],
   "source": [
    "docs_test_lemma = clean_text(docs_test_lemma, Clean_arg.LEMMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"train\", [word.str(\"tag\") for word in docs_train_lemma][0], \"test\", [word.str(\"tag\") for word in docs_test_lemma][0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_train_lower = copy.deepcopy(docs_train_lemma)\n",
    "docs_test_lower = copy.deepcopy(docs_test_lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "debug"
    ]
   },
   "outputs": [],
   "source": [
    "docs_train_lower = clean_text(docs_train_lower, Clean_arg.LOWERCASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "debug"
    ]
   },
   "outputs": [],
   "source": [
    "docs_test_lower = clean_text(docs_test_lower, Clean_arg.LOWERCASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"train\", [word.str(\"token\") for word in docs_train_lower][0], \"test\", [word.str(\"token\") for word in docs_test_lower][0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_train, docs_test = docs_train_lower, docs_test_lower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_words_train = [doc.words for doc in docs_train if is_positive(doc.rating)]\n",
    "negative_words_train = [doc.words for doc in docs_train if is_negative(doc.rating)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_words_train = [item for l in positive_words_train for item in l]\n",
    "negative_words_train = [item for l in negative_words_train for item in l]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_words_train[:10], negative_words_train[:10]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frenquency distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_fd_train = nltk.FreqDist(positive_words_train)\n",
    "negative_fd_train = nltk.FreqDist(negative_words_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_fd_train, negative_fd_train"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove common words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_set = set(positive_fd_train).intersection(negative_fd_train)\n",
    "\n",
    "for word in common_set:\n",
    "    del positive_fd_train[word]\n",
    "    del negative_fd_train[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_fd_train, negative_fd_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_100_positive_train = {word for word, count in positive_fd_train.most_common(100)}\n",
    "top_100_negative_train = {word for word, count in negative_fd_train.most_common(100)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_100_positive_train, top_100_negative_train"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = {\n",
    "    \"BernoulliNB\": BernoulliNB(),\n",
    "    \"ComplementNB\": ComplementNB(),\n",
    "    \"MultinomialNB\": MultinomialNB(),\n",
    "    \"KNeighborsClassifier\": KNeighborsClassifier(),\n",
    "    \"DecisionTreeClassifier\": DecisionTreeClassifier(),\n",
    "    \"RandomForestClassifier\": RandomForestClassifier(),\n",
    "    \"LogisticRegression\": LogisticRegression(),\n",
    "    \"MLPClassifier\": MLPClassifier(max_iter=1000),\n",
    "    \"AdaBoostClassifier\": AdaBoostClassifier(),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suf\n",
    "trinset, testset = train_test_split(doc, test_size=0.2, random_state=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12 (main, Apr  4 2022, 05:22:27) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bef2fa2dc83e4ccd7ad2aec876ba177966ddc8076da8c7972b1992baac764220"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
