{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text mining project\n",
    "## Goals\n",
    "• Gain practical experience with the complete data mining process\n",
    "\n",
    "• Get to know additional problem-specific\n",
    "\n",
    "• Pre-processing methods\n",
    "\n",
    "• data mining methods\n",
    "## Expectation\n",
    "• Select an interesting data mining problem of your choice\n",
    "\n",
    "• Solve the problem using\n",
    "\n",
    "• the data mining methods that we have learned so far, including\n",
    "\n",
    "• proper parameter optimization\n",
    "\n",
    "• problem-specific pre-processing and smart feature creation\n",
    "\n",
    "• additional data mining methods which might be helpful for solving the problem"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importations."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "from nltk.tag import pos_tag\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from nltk.probability import FreqDist"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.model_selection import learning_curve, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "## Classifier\n",
    "from sklearn.naive_bayes import (\n",
    "    BernoulliNB,\n",
    "    ComplementNB,\n",
    "    MultinomialNB,\n",
    ")\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, roc_curve, confusion_matrix, classification_report\n",
    "from sklearn.inspection import DecisionBoundaryDisplay"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup # suppression des balises html.\n",
    "import codecs # lire les fichiers.\n",
    "import contractions\n",
    "import copy\n",
    "import os # manipulation des fichiers et des dossiers.\n",
    "import re # utilisation de regex.\n",
    "import sys # quitter le programme en cas d'erreur.\n",
    "import string # suppression de la ponctuation.\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # DataFrame, ...\n",
    "import numpy as np # array, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualistion\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variables globales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG = False\n",
    "TEST = 3000\n",
    "MAX_FEATURES = 1500\n",
    "OUT_CSV = \"./out/csv/\"\n",
    "OUT_IMG = \"./out/img/\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fonctions."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outils."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lecture des fichiers et création des *datasets*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(directory):\n",
    "\t\"\"\"\n",
    "\tRenvoie un dataframe avec les données lu dans le fichier pointe par \"directory\".\n",
    "\t@param directory: le repertoire ou se trouve les fichiers à lire.\n",
    "\t@return dataset: contenant les données du fichier. \n",
    "\t\"\"\"\n",
    "\tdata = []\n",
    "\n",
    "\t# Files counter.\n",
    "\tfile_count = 0\n",
    "\n",
    "\t# Loop for files.\n",
    "\tfor filename in os.listdir(directory):\n",
    "\t\t\n",
    "\t\tfile = os.path.join(directory, filename)\n",
    "\t\tif os.path.isfile(file):\n",
    "\t\t\twith codecs.open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "\t\t\t\t\n",
    "\t\t\t\t# File name parsing to get id and rating.\n",
    "\t\t\t\tsplit_extension = filename.split(\".\")\n",
    "\t\t\t\tsplit_id_rating = split_extension[0].split(\"_\")\n",
    "\t\t\t\tid_str = split_id_rating[0]\n",
    "\t\t\t\trating_str = split_id_rating[1]\n",
    "\t\t\t\trating = -1\n",
    "\n",
    "\t\t\t\ttry:\n",
    "\t\t\t\t\trating = int(rating_str)\n",
    "\t\t\t\texcept ValueError:\n",
    "\t\t\t\t\tsys.exit(\"Error casting rating to int\")\n",
    "\n",
    "\t\t\t\treview = f.read()\n",
    "\t\t\t\tsentiment = 1 if directory.__contains__(\"pos\") else 0\n",
    "\t\t\t\tdata.append([review, rating, sentiment])\n",
    "\t\t\t\tfile_count += 1\n",
    "\t\t\t\t\n",
    "\tprint(\"file_count : {}\".format(file_count))\n",
    "\treturn pd.DataFrame(data, columns=[\"review\", \"rating\", \"sentiment\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Preprocessing*."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remplace les contractions englaises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_contraction(text):\n",
    "\treturn contractions.fix(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppression des balises html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_html(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    return soup.get_text()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppresion de la ponctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(text):\n",
    "    return text.translate(str.maketrans('', '', string.punctuation))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(text):\n",
    "\t# lemmatisation des mots.\n",
    "\t## si c'est un verbre on le mets à l'infinif.\n",
    "\t## si c'est un nom propre on le supprime.\n",
    "\t## pos_tag donne le type de chaque mot.\n",
    "\tverbs = [\"VBP\", \"VBN\", \"VBG\", \"VBD\", \"VB\", \"VBZ\"]\n",
    "\tproper_noun = [\"NNS\", \"NNP\", \"NNPS\"]\n",
    "\ttext = ' '.join(\n",
    "\t\t[lemmatizer.lemmatize(word, pos = \"v\") if tag in verbs \n",
    "\t\telse lemmatizer.lemmatize(word) \n",
    "\t\tfor word, tag in pos_tag(word_tokenize(text)) \n",
    "\t\tif tag not in proper_noun])\n",
    "\treturn text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppresion des *stop words*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "\ttokens = word_tokenize(text)\n",
    "\tfiltered_tokens = [token for token in tokens if token.lower() not in stop_words]\n",
    "\tfiltered_text = ' '.join(filtered_tokens)  \n",
    "\treturn filtered_text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppression de caractères spéciaux et des chiffres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_special_characters(text):\n",
    "    pattern = r'[^a-zA-z\\s]'\n",
    "    text = re.sub(pattern, '', text)\n",
    "    return text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppression des mots inutiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_usless_word(text):\n",
    "\ttokens = word_tokenize(text)\n",
    "\tusless_word = [\"movie\", \"film\", \"one\", \"story\"]\n",
    "\tfiltered_tokens = [token for token in tokens if token.lower() not in usless_word]\n",
    "\tfiltered_text = ' '.join(filtered_tokens)\n",
    "\treturn filtered_text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppression des mots communs entre les *positive reviews* et *negative reviews*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_common_word(df):\n",
    "\t\"\"\"\n",
    "\tSupprime les mots communs entre les textes positifs et les textes negatifs.\n",
    "\t@param df: un Dataframe contenant les textes.\n",
    "\t@return Dataframe contenant les textes sans les mots communs.\n",
    "\t\"\"\"\n",
    "\n",
    "\t# decoupage des textes positifs et des textes negatifs.\n",
    "\tdf_positive_words = df[df[\"sentiment\"] == 1]\n",
    "\tdf_negative_words = df[df[\"sentiment\"] == 0]\n",
    "\tpositive_reviews = df_positive_words[\"review\"].values\n",
    "\tnegative_reviews = df_negative_words[\"review\"].values\n",
    "\n",
    "\t#\n",
    "\tpositive_words = [word_tokenize(review) for review in positive_reviews]\n",
    "\tnegative_words = [word_tokenize(review) for review in negative_reviews]\n",
    "\tpositive_words_flatten = [word for word in positive_words for word in word]\n",
    "\tnegative_words_flatten = [word for word in negative_words for word in word]\n",
    "\n",
    "\t# construction de set avec chaque mot et leur frequence.\n",
    "\tpositive_fd = set(FreqDist(positive_words_flatten))\n",
    "\tnegative_fd = set(FreqDist(negative_words_flatten))\n",
    "\n",
    "\t# calcul des mots communs.\n",
    "\tcommon_set = positive_fd.intersection(negative_fd)\n",
    "\n",
    "\t# suppression des mots communs.\n",
    "\told_reviews = df[\"review\"].values\n",
    "\tnew_reviews = []\n",
    "\tnew_sentiment = []\n",
    "\twords_removed = 0\n",
    "\tfor index, review in enumerate(old_reviews):\n",
    "\t\tnew_review = []\n",
    "\t\tfor word in review.split():\n",
    "\t\t\tif word not in common_set:\n",
    "\t\t\t\tnew_review.append(word)\n",
    "\t\t\telse:\n",
    "\t\t\t\twords_removed += 1\n",
    "\t\tif len(new_review) != 0:\n",
    "\t\t\tnew_reviews.append(\" \".join(word for word in new_review))\n",
    "\t\t\tnew_sentiment.append(df.iloc[index][\"sentiment\"])\n",
    "\n",
    "\t# affichage.\n",
    "\tprint(\"{} words removed !\".format(words_removed))\n",
    "\t\n",
    "\tdata = {\"review\": new_reviews, \"sentiment\": new_sentiment}\n",
    "\tnew_df = pd.DataFrame(data)\n",
    "\treturn new_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Data visualization*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_most_common_words(df, name):\n",
    "\tall_words = []\n",
    "\tfor comment in df['review']:\n",
    "\t\twords = word_tokenize(comment)\n",
    "\t\tall_words.extend(words)\n",
    "\n",
    "\tfdist = FreqDist(all_words)\n",
    "\tnb_common_word = 50\n",
    "\n",
    "\twords = [word[0] for word in fdist.most_common(nb_common_word)]\n",
    "\tcounts = [word[1] for word in fdist.most_common(nb_common_word)]\n",
    "\n",
    "\tplt.figure(figsize=(15,5))\n",
    "\tplt.bar(words, counts)\n",
    "\tplt.xlabel('Words')\n",
    "\tplt.ylabel('Counts')\n",
    "\tplt.title(f'{nb_common_word} Most Common Words')\n",
    "\tplt.xticks(rotation=90)\n",
    "\tplt.savefig(f\"{OUT_IMG}plot_most_common_word_{name}.png\", bbox_inches='tight')\n",
    "\tplt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_word_cloud(df, name):\n",
    "\ttext = \" \".join(review for review in df.review)\n",
    "\n",
    "\t# création du word cloud\n",
    "\twordcloud = WordCloud(width = 800, height = 800,\n",
    "                background_color ='white',\n",
    "                min_font_size = 10).generate(text)\n",
    "\t\n",
    "\t# affichage de l image                    \n",
    "\tplt.figure(figsize = (5, 5), facecolor = None)\n",
    "\tplt.imshow(wordcloud)\n",
    "\tplt.axis(\"off\")\n",
    "\tplt.tight_layout(pad = 0)\n",
    "\tplt.savefig(f\"{OUT_IMG}word_cloud_{name}.png\", bbox_inches='tight')\n",
    "\tplt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrainement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entrainement(classifiers, X_train, X_test, y_train, y_test):\n",
    "\t\"\"\"\n",
    "\tEntraine les modeles et predits les classes.\n",
    "\t@param classifiers: dictionnaire contenant le nom et un modele de classifications.\n",
    "\t@param X_train: les donnees du train set.\n",
    "\t@param X_test: les donnees du test set.\n",
    "\t@pamam y_train: les classes du train set.\n",
    "\t@param y_test: les classes du test set.\n",
    "\t@return liste_predictions: une liste contenant le nom, le modele entraine et les predictions.\n",
    "\t\"\"\"\n",
    "\tliste_predictions = []\n",
    "\t\n",
    "\tfor nom, classifier in classifiers.items():\n",
    "\t\tprint(nom)\n",
    "\n",
    "\t\t# entrainement des modeles.\n",
    "\t\tprint(\"fitting...\", end=\"\")\n",
    "\t\tpipeline_classifier = Pipeline([\n",
    "\t\t\t(\"vectorize\", CountVectorizer(max_features=MAX_FEATURES)),\n",
    "\t\t\t(\"tfidf\", TfidfTransformer()),\n",
    "\t\t\t(\"classifier\", classifier),\n",
    "\t\t])\n",
    "\t\tpipeline_classifier = pipeline_classifier.fit(X_train, y_train)\n",
    "\t\tprint(\"Done\")\n",
    "\n",
    "\t\t# prediction des classes du test set.\n",
    "\t\tprint(\"predicting labels...\", end=\"\")\n",
    "\t\ty_pred_test = pipeline_classifier.predict(X_test)\n",
    "\t\tprint(\"Done\")\n",
    "\t\n",
    "\t\tliste_predictions.append((nom, pipeline_classifier, y_pred_test))\n",
    "\t\tprint(\"\")\n",
    "\t\n",
    "\treturn liste_predictions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rapport de classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_classification_report_plot(model_name, df, class_name, pred, y_test, figsize):\n",
    "\t\"\"\"\n",
    "\tAffiche le rapport de classification.\n",
    "\t@param model_name: le nom du modele.\n",
    "\t@param df: le Dataframe avec les textes.\n",
    "\t@param class_name: le nom de la classe.\n",
    "\t@param pred: le tableau de prediction.\n",
    "\t@param y_test: les classes du test set.\n",
    "\t@param figsize: la taille de la figure.\n",
    "\t\"\"\"\n",
    "\t\n",
    "\tlabels = np.unique(df[class_name].values)\n",
    "\tclass_report = classification_report(y_true=y_test, y_pred=pred, target_names=labels, output_dict=True)\n",
    "\tplt.figure(figsize=figsize)\n",
    "\tsns.heatmap(pd.DataFrame(class_report).iloc[:-1, :].T, annot=True)\n",
    "\tplt.title(f\"Rapport de classification pour {model_name}\")\n",
    "\tplt.savefig(f\"{OUT_IMG}classification_raport_{model_name}.png\", bbox_inches='tight')\n",
    "\tplt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_classification_report(model_name, pred, y_test):\n",
    "\t\"\"\"\n",
    "\tAffiche le rapport de classification.\n",
    "\t@param model_name: le nom du modele.\n",
    "\t@param pred: le tableau de prediction.\n",
    "\t@param y_test: les classes du test set.\n",
    "\t\"\"\"\n",
    "\t\n",
    "\tprint(\"Rapport de classification pour {}\".format(model_name))\n",
    "\tprint(classification_report(y_true=y_test, y_pred=pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrice de confusion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_confusion_matrix(model_name, df, class_name, pred, y_test, figsize):\n",
    "    \"\"\"\n",
    "\tAffiche le rapport de classification.\n",
    "\t@param model_name: le nom du modele.\n",
    "\t@param df: le Dataframe avec les textes.\n",
    "\t@param class_name: le nom de la classe.\n",
    "\t@param pred: le tableau de prediction.\n",
    "\t@param y_test: les classes du test set.\n",
    "\t@param figsize: la taille de la figure.\n",
    "\t\"\"\"\n",
    "    \n",
    "    labels = np.unique(df[class_name].values)\n",
    "    conf_matrix = confusion_matrix(y_test, pred, labels=labels)\n",
    "    df_conf_matrix = pd.DataFrame(conf_matrix, columns=labels)\n",
    "    df_conf_matrix[\"index\"] = labels\n",
    "    df_conf_matrix = df_conf_matrix.set_index(\"index\")\n",
    "\n",
    "    plt.figure(figsize=figsize)\n",
    "    sns.heatmap(df_conf_matrix, annot=True, fmt=\"d\", cmap=\"coolwarm\")\n",
    "    plt.title(f\"Matrice de confusion pour {model_name}\")\n",
    "    plt.savefig(f\"{OUT_IMG}confusion_matrix_{model_name}.png\", bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Courbe d'apprentissage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_learning_curve(model_name, model, x_train, y_train, figsize):\n",
    "    \"\"\"\n",
    "\tAffiche le rapport de classification.\n",
    "\t@param model_name: le nom du modele.\n",
    "\t@param model: le modele de classification.\n",
    "    @param x_train: les donnees du train set.\n",
    "    @param y_train: les classes du train set.\n",
    "    @param figsize: la taille de la figure.\n",
    "\t\"\"\"\n",
    "\n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        estimator=model, X=x_train, y=y_train, \n",
    "        train_sizes=np.linspace(0.1, 1.0, 4),\n",
    "        n_jobs=10, verbose=0, random_state=42)\n",
    "\n",
    "    #\n",
    "    # Calculate training and test mean and std\n",
    "    #\n",
    "    train_mean = np.mean(train_scores, axis=1)\n",
    "    train_std = np.std(train_scores, axis=1)\n",
    "    test_mean = np.mean(test_scores, axis=1)\n",
    "    test_std = np.std(test_scores, axis=1)\n",
    "\n",
    "    #\n",
    "    # Plot the learning curve\n",
    "    #\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.plot(train_sizes, train_mean, color='blue', marker='o', markersize=5, label='Training Accuracy')\n",
    "    plt.fill_between(train_sizes, train_mean + train_std, train_mean - train_std, alpha=0.15, color='blue')\n",
    "    plt.plot(train_sizes, test_mean, color='green', marker='+', markersize=5, linestyle='--', label='Validation Accuracy')\n",
    "    plt.fill_between(train_sizes, test_mean + test_std, test_mean - test_std, alpha=0.15, color='green')\n",
    "    plt.title(\"Learning curve pour {}\".format(model_name))\n",
    "    plt.xlabel('Training Data Size')\n",
    "    plt.ylabel('Model accuracy')\n",
    "    plt.grid()\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.savefig(f\"{OUT_IMG}learning_curve_{model_name}.png\", bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Roc curve*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_curve(model_name, model_fit, X_test, y_test, figsize): \n",
    "\t\"\"\"\n",
    "\tAffiche le rapport de classification.\n",
    "\t@param model_name: le nom du modele.\n",
    "\t@param model_fit: le modele de classification.\n",
    "\t@param X_test: les donnees du test set.\n",
    "\t@param y_test: les classes du test set.\n",
    "\t@param figsize: la taille de la figure.\n",
    "\t\"\"\"\n",
    "\n",
    "\tprobs = model_fit.predict_proba(X_test)  \n",
    "\tprobs = probs[:, 1]\n",
    "\tfper, tper, thresholds = roc_curve(y_test, probs) \n",
    "\n",
    "\tplt.figure(figsize=figsize)\n",
    "\tplt.plot(fper, tper, color='orange', label='ROC')\n",
    "\tplt.plot([0, 1], [0, 1], color='darkblue', linestyle='--')\n",
    "\tplt.xlabel('False Positive Rate')\n",
    "\tplt.ylabel('True Positive Rate')\n",
    "\tplt.title(\"ROC curve pour {}\".format(model_name))\n",
    "\tplt.savefig(f\"{OUT_IMG}roc_curve_{model_name}.png\", bbox_inches='tight')\n",
    "\tplt.legend()\n",
    "\tplt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation global."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(liste_prediction, df, class_name, X_train, X_test, y_train, y_test):\n",
    "\t\"\"\"\n",
    "\tAffiche le rapport de classification.\n",
    "\t@param liste_prediction: la liste contenant les modeles.\n",
    "\t@param df: le Dataframe avec les textes.\n",
    "\t@param class_name: le nom de la classe.\n",
    "\t@param X_train: les donnees du train set.\n",
    "\t@param X_test: les donnees du test set.\n",
    "\t@param y_train: les classes du train set.\n",
    "\t@param y_test: les classes du test set.\n",
    "\t\"\"\"\n",
    "\tfor nom, model_fit, y_pred_test in liste_prediction:\n",
    "\t\tfigsize = (4,3)\n",
    "\t\tmy_classification_report(nom, y_pred_test, y_test)\n",
    "\n",
    "\t\tmy_confusion_matrix(nom, df, class_name, y_pred_test, y_test, figsize)\n",
    "\n",
    "\t\tmy_learning_curve(nom, model_fit, X_train, y_train, figsize)\n",
    "\n",
    "\t\tplot_roc_curve(nom, model_fit, X_test, y_test, figsize)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture des fichiers et création des *datasets*."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Trainset*."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Positive reviews*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_pos = create_dataset(\"./data/train/pos/\")\n",
    "df_train_pos"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Negative reviews*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_neg = create_dataset(\"./data/train/neg/\")\n",
    "df_train_neg"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Création du *trainset*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.concat([df_train_pos, df_train_neg], axis=0)\n",
    "df_train.reset_index(drop=True, inplace=True)\n",
    "df_train"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ecriture dans un fichier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.to_csv(f\"{OUT_CSV}df_train.csv\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Testset*."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Positive reviews*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_pos = create_dataset(\"./data/test/pos/\")\n",
    "df_test_pos"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Negative reviews*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_neg = create_dataset(\"./data/test/neg/\")\n",
    "df_train_neg"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Création du *testset*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.concat([df_test_pos, df_test_neg], axis=0)\n",
    "df_test.reset_index(drop=True, inplace=True)\n",
    "df_test"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ecriture dans un fichier externe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.to_csv(f\"{OUT_CSV}df_test.csv\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Preprocessing*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_prepro = copy.deepcopy(df_train)\n",
    "df_test_prepro = copy.deepcopy(df_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Trainset*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEBUG:\n",
    "\tdf_train_prepro = df_train.sample(TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_prepro[\"review\"] = df_train_prepro[\"review\"].apply(strip_html)\n",
    "df_train_prepro.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_prepro[\"review\"] = df_train_prepro[\"review\"].apply(remove_contraction)\n",
    "df_train_prepro.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_prepro[\"review\"] = df_train_prepro[\"review\"].apply(remove_punctuation)\n",
    "df_train_prepro.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_prepro[\"review\"] = df_train_prepro[\"review\"].apply(lemmatize)\n",
    "df_train_prepro.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_prepro[\"review\"] = df_train_prepro[\"review\"].apply(remove_stopwords)\n",
    "df_train_prepro.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_prepro[\"review\"] = df_train_prepro[\"review\"].apply(remove_special_characters)\n",
    "df_train_prepro.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_prepro[\"review\"] = df_train_prepro[\"review\"].apply(remove_usless_word)\n",
    "df_train_prepro.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ecriture dans un fichier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_prepro.to_csv(f\"{OUT_CSV}df_train_prepro.csv\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Testset*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEBUG:\n",
    "\tdf_test_prepro = df_test_prepro.sample(TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_prepro[\"review\"] = df_test_prepro[\"review\"].apply(strip_html)\n",
    "df_test_prepro.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_prepro[\"review\"] = df_test_prepro[\"review\"].apply(remove_contraction)\n",
    "df_test_prepro.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_prepro[\"review\"] = df_test_prepro[\"review\"].apply(remove_punctuation)\n",
    "df_test_prepro.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_prepro[\"review\"] = df_test_prepro[\"review\"].apply(lemmatize)\n",
    "df_test_prepro.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_prepro[\"review\"] = df_test_prepro[\"review\"].apply(remove_stopwords)\n",
    "df_test_prepro.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_prepro[\"review\"] = df_test_prepro[\"review\"].apply(remove_special_characters)\n",
    "df_test_prepro.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_prepro[\"review\"] = df_test_prepro[\"review\"].apply(remove_usless_word)\n",
    "df_test_prepro.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ecriture dans un fichier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_prepro.to_csv(f\"{OUT_CSV}df_test_prepro.csv\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Data visualization*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_visu = pd.read_csv(f\"{OUT_CSV}df_train_prepro.csv\")\n",
    "df_test_visu = pd.read_csv(f\"{OUT_CSV}df_test_prepro.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mots plus communs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_most_common_words(df_train_visu, \"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_visu2 = copy.deepcopy(df_train_visu)\n",
    "df_train_visu2 = remove_common_word(df_train_visu2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_pos_visu = df_train_visu2[df_train_visu2[\"sentiment\"] == 1]\n",
    "plot_most_common_words(df_train_pos_visu, \"train_pos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_neg_visu = df_train_visu2[df_train_visu2[\"sentiment\"] == 0]\n",
    "plot_most_common_words(df_train_neg_visu, \"train_neg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_most_common_words(df_test_visu, \"test\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nuage de mots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_word_cloud(df_train_visu, \"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_word_cloud(df_test_visu, \"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_classify = pd.read_csv(f\"{OUT_CSV}df_train_prepro.csv\")\n",
    "df_test_classify = pd.read_csv(f\"{OUT_CSV}df_test_prepro.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = {\n",
    "    \"LogisticRegression\": LogisticRegression(), # 87%\n",
    "    \"MultinomialNB\": MultinomialNB(), # 85%\n",
    "    \"ComplementNB\": ComplementNB(), # 85%\n",
    "    \"BernoulliNB\": BernoulliNB(), # 84%\n",
    "    # \"RandomForestClassifier\": RandomForestClassifier(), # 83%\n",
    "    # \"AdaBoostClassifier\": AdaBoostClassifier(), # 80%\n",
    "    # \"KNeighborsClassifier\": KNeighborsClassifier(), # 71%\n",
    "    # \"DecisionTreeClassifier\": DecisionTreeClassifier(), # 70%\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Découpage en X et y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = df_train_classify[\"review\"].values, df_train_classify[\"sentiment\"].values\n",
    "X_test, y_test = df_test_classify[\"review\"].values, df_test_classify[\"sentiment\"].values"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paramètre par défaut."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrainement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "liste_predictions = entrainement(classifiers, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation(liste_predictions, df_test_classify, \"sentiment\", X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import eli5\n",
    "# from eli5.lime import TextExplainer\n",
    "\n",
    "# text = df_test_classify.review.to_list()[0]\n",
    "# pipeline = liste_predictions[0][1]\n",
    "# prediction = pipeline.predict([text])[0]\n",
    "\n",
    "# explainer = TextExplainer(random_state=0)\n",
    "# explainer.fit(text, pipeline.predict_proba)\n",
    "\n",
    "# print(\"Prediction:\", df_test_classify.sentiment[prediction])\n",
    "# print(\"Explanation:\")\n",
    "# explainer.show_prediction(target_names=df_test_classify.sentiment.tolist())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparaison."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrice de confusion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noms = []\n",
    "vrai_positif = []\n",
    "vrai_negatif = []\n",
    "for nom, _, y_pred_test in liste_predictions:\n",
    "\tlabels = np.unique(df_train_classify[\"sentiment\"].values)\n",
    "\tconf_matrix = confusion_matrix(y_test, y_pred_test, labels=labels)\n",
    "\n",
    "\tnoms.append(nom)\n",
    "\tvrai_positif.append(conf_matrix[0][0])\n",
    "\tvrai_negatif.append(conf_matrix[1][1])\n",
    "\n",
    "data = {\"nom\": noms, \"vrai_positif\": vrai_positif, \"vrai_negatif\": vrai_negatif}\n",
    "df_matric_confu = pd.DataFrame(data, columns=[\"nom\", \"vrai_positif\", \"vrai_negatif\"])\n",
    "df_matric_confu.plot.bar(x=\"nom\", rot=30, title=\"Comparaison des vrais positifs et des vrais negatifs de chaque algorithme\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig(f\"{OUT_IMG}comparaison_matrice_confusion.png\", bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *ROC curve*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,5))\n",
    "for index, (nom, model_fit, _) in enumerate(liste_predictions):\n",
    "\tcolors = [\"blue\", \"red\", \"green\", \"yellow\"]\n",
    "\tprobs = model_fit.predict_proba(X_test) \n",
    "\tprobs = probs[:, 1]\n",
    "\tfper, tper, thresholds = roc_curve(y_test, probs) \n",
    "\tplt.plot(fper, tper, color=colors[index], label=f\"{nom}\")\n",
    "\n",
    "plt.plot([0, 1], [0, 1], color='darkblue', linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title(\"Comparaison des ROC curve de chaque algorithme\")\n",
    "plt.legend()\n",
    "plt.savefig(f\"{OUT_IMG}comparaison_roc_curve.png\", bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Amélioration des paramètres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Pipeline([\n",
    "\t(\"vectorize\", CountVectorizer()),\n",
    "\t(\"tfidf\", TfidfTransformer()),\n",
    "\t(\"logistic\", LogisticRegression(max_iter=1000)),\n",
    "])\n",
    "\n",
    "max_features = [1000, 1500, 3000]\n",
    "c = [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "tdidf = [\"l1\",\"l2\"]\n",
    "\n",
    "param_grid_lbfgs_sag = {\n",
    "\t\"vectorize__max_features\": max_features,\n",
    "\t\"tfidf__norm\": tdidf,\n",
    "\t\"logistic__solver\": [\"lbfgs\", \"sag\"],\n",
    "\t\"logistic__penalty\": [\"l2\"],\n",
    "\t\"logistic__C\": c\n",
    "}\n",
    "\n",
    "param_grid_saga= {\n",
    "\t\"vectorize__max_features\": max_features,\n",
    "\t\"tfidf__norm\": tdidf,\n",
    "\t\"logistic__solver\": [\"saga\"],\n",
    "\t\"logistic__penalty\": [\"elasticnet\", \"l1\", \"l2\"],\n",
    "\t\"logistic__C\": c\n",
    "}\n",
    "\n",
    "param_grid = [param_grid_lbfgs_sag, param_grid_saga]\n",
    "grid = RandomizedSearchCV(model, param_grid, n_iter=100)\n",
    "# grid = GridSearchCV(model, param_grid)\n",
    "grid.fit(X_train, y_train)\n",
    "grid.best_estimator_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrainement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid.best_estimator_\n",
    "# Pipeline(steps=[('vectorize', CountVectorizer(max_features=3000)),\n",
    "#                 ('tfidf', TfidfTransformer()),\n",
    "#                 ('logistic', LogisticRegression(C=10, max_iter=1000, solver='sag'))])\n",
    "\n",
    "best_model = grid.best_estimator_\n",
    "\n",
    "print(\"fitting...\", end=\"\")\n",
    "best_model.fit(X_train, y_train)\n",
    "print(\"Done\")\n",
    "\n",
    "print(\"predicting labels...\", end=\"\")\n",
    "y_pred = best_model.predict(X_test)\n",
    "print(\"Done\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_classification_report(\"LogisticRegression\", y_pred, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f54c43c78a7239850f68dcdf5e10933261189269a0a228285fdc32d82cd4329e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
